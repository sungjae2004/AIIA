
# non-linear activation을 사용한 이진 분류
- 입력 : 키와 몸무게 & 출력 : 1(살 빼야 하는 사람) or 0(살 쪄야 하는 사람)
- step1. 데이터 모으기
- step2. 모델 만들기 : unit step fuction 이용해 분류, 입력 노드 2개 출력 노드 1개, hidden layer 없으면 퍼셉트론이라고 함
- step3. 모델 학습 시키기 : 분류 경계가 선형이 되면 선형 분류 
- step4. 모델 테스트하기 : 3D로 표현되는 것 확인, 입려과 출력과의 관계는 비선형
- 한계점 : 미분 불가, 너무 빡빡하게 분류 => sigmoid로 해결 가능

# sigmoid 함수
![image](https://github.com/user-attachments/assets/a6f7f9db-48c6-43ff-8512-82aeebefc2f2)

- 전구간 미분 가능, 좀 더 부드러운 분류 가능
- 확률(혹은 정도)로 해석 가능
- 가장 멀리 찢어 놓는 합리적인 분류 경계 선 찾게 됨

# sigmoid를 사용한 이진 분류
- 입력은 3100100인 (강or고) 사진이며 바로 출력 층으로 연결된다고 가정
- 파라미터 30001개 -> gradient길이 30001 -> Loss 정의 => BCE Loss (이진분류에 적합)
- 신경망이 사진을 입력 받아 강아지일 확률 출력 -> 강아지 사진을 넣었을 때 0에 가까운 값이 나올수록 Loss가 커지도록 함
- Loss로 '출력의 의미' 컨트롤
# 로지스틱 회귀분석 (Logistic Regression)

- 입력과 출력 사이의 관계를 확률 함수로 표현하고 이 함수를 은닉층이 없는 인공 신경망으로 놓고 추정하는 방법
- 분류 문제를 다루는데도 '회귀'라고 하는 이유는 분류와 회귀가 근본적으로 같은 접근 방식 공유하기 때문
---

### 특징
- **입력**: 1개 이상의 독립변수 \\( x \\)
- **출력**: 데이터가 두 범주 중 하나에 속할 확률
- 이진 분류 문제에 자주 사용됩니다.
---


##  로지스틱 회귀 모델의 장점

- 선형적으로 분리 가능한 데이터를 잘 처리함
- 계산 효율성이 높고, 확률로 해석 가능
- 이진 분류 문제에서 간단하고 효과적


# MLE (Maximum Likelihood Estimation)
- Loss를 최소화하는 파라미터를 찾는 딥러닝의 학습 과정
-BCE Loss와 MSE Loss 모두 Likelihood라는 공통된 뿌리이지만 가정한 분포가 다름
-Likelihood
   - 앞의 것을 보고 그 속에 숨겨져 있는 뒤에 것을 알아낸다 (조건부 확률 값이지미나 확률분포는 아님, 합 != 1
- BCE : 베르누이로 가정하고, 신경망이 'y1 = 1일 확률 q1'을 잘 출력하게끔 NLL을 loss로 삼고 학습
- MSE : 가루시안로 가정하고, 신경망이 '랜덤변수 y1의 평균'을 y1과 가깝게 나오도록 NLL을 loss로 삼고 학습

## 2. 주요 차이점

| **특징**        | **MSE**                              | **BCE**                              |
|------------------|--------------------------------------|---------------------------------------|
| **사용 목적**    | 주로 **회귀 문제**                  | 주로 **이진 분류 문제**              |
| **출력 값**      | 연속적인 값 (예: 0.5, 2.3 등)        | 확률 값 (0에서 1 사이)               |
| **손실 해석**    | 오차의 제곱합으로 손실 계산          | 로그 우도로 확률 분포 간 차이를 계산 |
| **예측 값 요구** | 일반적인 실수 값                     | 0에서 1 사이의 확률 값               |
| **민감도**       | 이상치(outlier)에 민감               | 확률 분포의 변화에 민감              |
| **경사 하강법**  | 선형 관계에서 학습 속도가 빠름       | 비선형 분포에 적합                   |

---

### **MSE**
**장점**:
- 계산이 간단하며, 대부분의 회귀 문제에 효과적.
- 오차가 연속적이고 대칭적일 때 잘 작동.

**단점**:
- 이상치에 매우 민감 (오차를 제곱하기 때문).
- 분류 문제에서는 확률적 출력을 다루기 어려움.

### **BCE**
**장점**:
- 확률 기반 출력(예: 시그모이드 또는 소프트맥스)에 적합.
- 분류 문제의 성능을 정확히 측정 가능.

**단점**:
- 계산량이 더 많으며, 손실 값이 MSE보다 복잡하게 변화.
- 모델 출력 값이 정확히 0 또는 1에 가까울 경우, 계산에서 로그에 의해 손실이 커질 수 있음.

---

## 4. 적용 상황

| 문제 유형           | 적합한 손실 함수  |
|---------------------|-------------------|
| **회귀 문제**       | MSE              |
| **이진 분류 문제**  | BCE              |
| **확률 기반 분류 문제** | BCE              |
| **비선형 관계 데이터** | BCE              |

---

# 다중분류
- 입력이 3100100인 (강or고or소) 사진이라면 확률 3개가 필요
- 파라미터 90003개 -> 그라디언트 길이 90003
- 출력 노드의 수를 늘려서 각 노드가 각 동물을 담당하도록 정답(label) y는 강[1:0:0] 고[0:1:0] 소[0:0:1] => ont-hot encoding
- 확률 분포가 출력되도록 마지막 액티베이션은 softmax 사용
- MLE로 생각하면 멀티누이(또는 카테고리컬) 분포를 따를 것이다
- ont-hot 고려하면 0 <= -logq 즉, 한놈만 팬다 (하나 1로하면 나머지 자동으로 0)

## CE
- 레이블이 카테고리분포(멀티누이)를 따른다고 가정하고 NLL을 구하며 이를 통해 얻은 Loss
- 항상 엔트로피보다 크거나 같음
- CE를 줄일수록 머신의 출력이 label과 가까워짐

+ step1. 입,출력 정의 (회귀, 다중분류, multi-label)
+ step2. model 만들기 (MLP, CNN, RNN)
+ step3. Loss 정의 (회귀-MSE, 이진-BCE, 다중-CE, 사실은 다 NLL)
+ step4. weights 최적화 (SGD, Adam)
