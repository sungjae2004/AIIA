
# non-linear activation을 사용한 이진 분류
- 입력 : 키와 몸무게 & 출력 : 1(살 빼야 하는 사람) or 0(살 쪄야 하는 사람)
- step1. 데이터 모으기
- step2. 모델 만들기 : unit step fuction 이용해 분류, 입력 노드 2개 출력 노드 1개, hidden layer 없으면 퍼셉트론이라고 함
- step3. 모델 학습 시키기 : 분류 경계가 선형이 되면 선형 분류 
- step4. 모델 테스트하기 : 3D로 표현되는 것 확인, 입려과 출력과의 관계는 비선형
- 한계점 : 미분 불가, 너무 빡빡하게 분류 => sigmoid로 해결 가능

# sigmoid 함수
![image](https://github.com/user-attachments/assets/a6f7f9db-48c6-43ff-8512-82aeebefc2f2)

- 전구간 미분 가능, 좀 더 부드러운 분류 가능
- 확률(혹은 정도)로 해석 가능
- 가장 멀리 찢어 놓는 합리적인 분류 경계 선 찾게 됨

# sigmoid를 사용한 이진 분류
- 
# 로지스틱 회귀분석 (Logistic Regression)

로지스틱 회귀분석은 데이터가 특정 범주에 속할 확률을 0에서 1 사이의 값으로 예측하고, 그 확률에 따라 가능성이 더 높은 범주에 속하는 것으로 분류해 주는 **지도 학습 알고리즘**입니다.

---

## 1. 로지스틱 회귀란?

로지스틱 회귀는 선형 회귀 분석과 비슷하지만, 결과값 \\( Y \\)가 확률로 표현되므로 하한과 상한이 \\( [0, 1] \\) 사이로 정해져 있습니다. 특정 **임계값(Threshold)**를 기준으로 확률값을 이진 분류(binary classification)로 변환합니다.

### 특징
- **입력**: 1개 이상의 독립변수 \\( x \\)
- **출력**: 데이터가 두 범주 중 하나에 속할 확률
- 이진 분류 문제에 자주 사용됩니다.

---

## 2. 로지스틱 회귀의 수식

로지스틱 회귀 모델은 다음과 같이 나타낼 수 있습니다.

### (1) Log Odds
종속변수가 0일 확률 \\( P(y=0|x) \\)을 구하는 식은 아래와 같습니다:

\\[
\log \text{Odds} = \log \left( \frac{P(y=0|x)}{1 - P(y=0|x)} \right) = \beta_0 + \sum_{j=1}^p \beta_j x_j
\\]

- **Odds**: 사건 발생 확률을 발생하지 않을 확률로 나눈 값
  \\[
  \text{Odds(Malignant)} = \frac{P(Y=0|x)}{1 - P(Y=0|x)} = 0.2 / 0.8 = 0.25
  \\]

- **Log Odds**: Odds 값에 로그를 취한 값

### (2) Log Odds의 확률 변환
Log Odds 값을 이용하여 \\( P(Y=0|x) \\)를 계산합니다.

\\[
P(y=0|x) = \frac{\exp\left( \beta_0 + \sum_{j=1}^p \beta_j x_j \right)}{1 + \exp\left( \beta_0 + \sum_{j=1}^p \beta_j x_j \right)}
\\]

이를 간단히 정리하면:

\\[
P(y=0|x) = \frac{1}{1 + \exp(-z)}, \quad \text{where } z = \beta_0 + \sum_{j=1}^p \beta_j x_j
\\]

---

## 3. 시그모이드 함수 (Sigmoid Function)

시그모이드 함수는 Log Odds 값을 0과 1 사이의 확률로 변환합니다. 함수의 형태는 아래와 같습니다:

\\[
\sigma(z) = \frac{1}{1 + \exp(-z)}
\\]

- **특징**:
  - \\( z \to \infty \\): \\( \sigma(z) \to 1 \\)
  - \\( z \to -\infty \\): \\( \sigma(z) \to 0 \\)

---

## 4. 로지스틱 회귀의 단계

로지스틱 회귀분석은 다음 단계를 거칩니다:

1. **실제 데이터를 대입**하여 Odds와 회귀 계수(\\( \beta \\))를 구합니다.
2. **Log Odds를 계산**한 후, 이를 시그모이드 함수의 입력으로 넣어 특정 범주에 속할 확률을 계산합니다.
3. 설정한 **Threshold 값**에 따라 확률값을 이진 분류로 변환합니다:
   - \\( P(y=1|x) > \text{threshold} \\): 클래스 1
   - \\( P(y=1|x) \leq \text{threshold} \\): 클래스 0

---

## 5. 로지스틱 회귀 모델의 장점

- 선형적으로 분리 가능한 데이터를 잘 처리함
- 계산 효율성이 높고, 확률로 해석 가능
- 이진 분류 문제에서 간단하고 효과적

---

## 6. 결론

로지스틱 회귀는 Log Odds를 계산한 뒤 이를 시그모이드 함수로 변환하여 확률값을 계산하고, 설정한 임계값에 따라 데이터를 분류하는 분류 모델입니다. 이 모델은 단순하면서도 강력한 성능을 제공하며, 특히 이진 분류 문제에서 널리 사용됩니다.


# MSE와 BCE 비교

MSE(Mean Squared Error)와 BCE(Binary Cross-Entropy)는 머신러닝 및 딥러닝 모델에서 자주 사용되는 손실 함수입니다. 아래는 이 두 손실 함수의 정의, 특징, 차이점 등을 비교한 내용입니다.

---

## 1. 정의

### **MSE (Mean Squared Error)**
- **공식**:
  \[
  	ext{MSE} = rac{1}{N} \sum_{i=1}^N \left( y_i - \hat{y}_i 
ight)^2
  \]
  - \( y_i \): 실제 값  
  - \( \hat{y}_i \): 예측 값  
  - \( N \): 데이터 샘플의 개수  

- **설명**:  
  - 예측 값과 실제 값 간의 차이(오차)를 제곱하여 평균화한 값.  
  - 회귀 문제에서 주로 사용되며, 예측 값이 연속적인 경우 적합.

### **BCE (Binary Cross-Entropy)**
- **공식**:
  \[
  	ext{BCE} = -rac{1}{N} \sum_{i=1}^N \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) 
ight]
  \]
  - \( y_i \): 실제 클래스 (0 또는 1)  
  - \( \hat{y}_i \): 예측 확률 (0에서 1 사이)  

- **설명**:  
  - 모델이 출력한 확률값과 실제 라벨 간의 불확실성을 측정.  
  - 이진 분류 문제에서 주로 사용되며, 확률 기반 출력에 적합.

---

## 2. 주요 차이점

| **특징**        | **MSE**                              | **BCE**                              |
|------------------|--------------------------------------|---------------------------------------|
| **사용 목적**    | 주로 **회귀 문제**                  | 주로 **이진 분류 문제**              |
| **출력 값**      | 연속적인 값 (예: 0.5, 2.3 등)        | 확률 값 (0에서 1 사이)               |
| **손실 해석**    | 오차의 제곱합으로 손실 계산          | 로그 우도로 확률 분포 간 차이를 계산 |
| **예측 값 요구** | 일반적인 실수 값                     | 0에서 1 사이의 확률 값               |
| **민감도**       | 이상치(outlier)에 민감               | 확률 분포의 변화에 민감              |
| **경사 하강법**  | 선형 관계에서 학습 속도가 빠름       | 비선형 분포에 적합                   |

---

## 3. 장단점

### **MSE**
**장점**:
- 계산이 간단하며, 대부분의 회귀 문제에 효과적.
- 오차가 연속적이고 대칭적일 때 잘 작동.

**단점**:
- 이상치에 매우 민감 (오차를 제곱하기 때문).
- 분류 문제에서는 확률적 출력을 다루기 어려움.

### **BCE**
**장점**:
- 확률 기반 출력(예: 시그모이드 또는 소프트맥스)에 적합.
- 분류 문제의 성능을 정확히 측정 가능.

**단점**:
- 계산량이 더 많으며, 손실 값이 MSE보다 복잡하게 변화.
- 모델 출력 값이 정확히 0 또는 1에 가까울 경우, 계산에서 로그에 의해 손실이 커질 수 있음.

---

## 4. 적용 상황

| 문제 유형           | 적합한 손실 함수  |
|---------------------|-------------------|
| **회귀 문제**       | MSE              |
| **이진 분류 문제**  | BCE              |
| **확률 기반 분류 문제** | BCE              |
| **비선형 관계 데이터** | BCE              |

---

## 5. 결론

- **MSE**는 회귀 문제에서 적합하며, 연속적인 출력 값을 다룰 때 사용됩니다.
- **BCE**는 확률 기반 예측을 위한 이진 분류 문제에서 사용되며, 분류 성능을 정확히 평가하는 데 적합합니다.

